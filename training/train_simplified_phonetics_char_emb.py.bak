import os
import shutil
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_from_disk
from torch.utils.data import DataLoader, Dataset
from collections import defaultdict
from tqdm import tqdm
import pickle

# Settings
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dataset_path = "../data/phonetic_wikitext_with_misspellings_simplified"
embedding_dir = "../results/trained_embeddings/charcnn_phonetics"
embedding_dim = 300
max_word_len = 20
batch_size = 128
epochs = 40

# Cleanup output dir
if os.path.exists(embedding_dir):
    print("Cleaning old embeddings directory...")
    shutil.rmtree(embedding_dir)
os.makedirs(embedding_dir, exist_ok=True)

# Load phonetic data
print("Loading phonetic dataset...")
dataset = load_from_disk(dataset_path)["train"]
corpus = [word for line in dataset["simplified_phonetic_text"] for word in line.split()]
unique_words = list(set(corpus))

# Character vocabulary
print("Building character vocabulary...")
char_set = set(c for word in unique_words for c in word)
char2idx = {c: i + 1 for i, c in enumerate(sorted(char_set))}
char2idx["<pad>"] = 0
idx2char = {i: c for c, i in char2idx.items()}

# Word dataset as char tensors
def word_to_tensor(word):
    chars = [char2idx.get(c, 0) for c in word[:max_word_len]]
    chars += [0] * (max_word_len - len(chars))
    return torch.tensor(chars[:max_word_len], dtype=torch.long)

class WordDataset(Dataset):
    def __init__(self, words):
        self.words = words
        self.tensors = [word_to_tensor(w) for w in words]
    def __len__(self):
        return len(self.words)
    def __getitem__(self, idx):
        return self.tensors[idx], self.words[idx]

# Model: CharCNN Encoder
class CharCNNEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_dim, out_dim):
        super().__init__()
        self.char_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=emb_dim, out_channels=out_dim, kernel_size=k)
            for k in [3, 4, 5]
        ])
    def forward(self, x):  # x: (batch, max_word_len)
        x = self.char_emb(x).transpose(1, 2)  # (batch, emb_dim, len)
        convs = [F.relu(conv(x)) for conv in self.convs]
        pooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]
        return torch.cat(pooled, dim=1)  # (batch, out_dim * len(kernels))

# Prepare data
word_ds = WordDataset(unique_words)
word_dl = DataLoader(word_ds, batch_size=batch_size, shuffle=True)

# Training setup
model = CharCNNEmbedding(vocab_size=len(char2idx), emb_dim=50, out_dim=100).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

print("Training character-level embeddings...")
model.train()
for epoch in range(epochs):
    total_loss = 0
    for batch, _ in tqdm(word_dl, desc=f"Epoch {epoch+1}/{epochs}"):
        batch = batch.to(device)
        optimizer.zero_grad()
        embeddings = model(batch)
        loss = embeddings.norm(dim=1).mean()  # Dummy loss: L2 norm (just to train encoder)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} Loss: {total_loss / len(word_dl):.4f}")

# Build word → embedding map
print("Generating final word embeddings...")
model.eval()
word_embeddings = {}
with torch.no_grad():
    for word in tqdm(unique_words):
        tensor = word_to_tensor(word).unsqueeze(0).to(device)
        emb = model(tensor).squeeze(0).cpu().numpy()
        word_embeddings[word] = emb

# Save embeddings
with open(os.path.join(embedding_dir, "charcnn_embeddings.pkl"), "wb") as f:
    pickle.dump(word_embeddings, f)

with open(os.path.join(embedding_dir, "charcnn_vocab.pkl"), "wb") as f:
    pickle.dump(char2idx, f)
from gensim.models import KeyedVectors
import numpy as np

# Save as KeyedVectors model (like FastText)
print("Saving embeddings as KeyedVectors (.model)...")
vector_size = next(iter(word_embeddings.values())).shape[0]
kv_model = KeyedVectors(vector_size=vector_size)

# Prepare word and vector list
words = list(word_embeddings.keys())
vectors = np.array([word_embeddings[w] for w in words])
kv_model.add_vectors(words, vectors)

# Save model in Gensim-compatible format
kv_model.save(os.path.join(embedding_dir, "charcnn_phonetics.model"))
kv_model.save_word2vec_format(os.path.join(embedding_dir, "charcnn_phonetics.kv"))
print(f"✅ Gensim KeyedVectors saved to: {embedding_dir}")

print(f"✅ CharCNN embeddings saved to: {embedding_dir}")
