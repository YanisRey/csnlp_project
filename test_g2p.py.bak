from datasets import load_dataset
from g2p_en import G2p
import random
import string
import nltk

# Ensure required POS tagger is downloaded
nltk.download('averaged_perceptron_tagger_eng', quiet=True)

# Load Wikitext dataset
print("Loading Wikitext...")
dataset = load_dataset("wikitext", "wikitext-103-v1")
text_data = ' '.join(dataset['train']['text'][:10000])
words = list(set(text_data.split()))

# Filter out non-alpha short words
valid_words = [w for w in words if w.isalpha() and len(w) > 3]
sampled_words = random.sample(valid_words, 100)

# Typo introduction
def introduce_single_typo(word):
    idx = random.randint(0, len(word) - 1)
    typo_char = random.choice([c for c in string.ascii_lowercase if c != word[idx].lower()])
    return word[:idx] + typo_char + word[idx+1:]

# G2P setup
g2p = G2p()

# Output
output_path = "g2p_typos_phonetics.txt"
with open(output_path, "w", encoding="utf-8") as f:
    for word in sampled_words:
        typo = introduce_single_typo(word)
        try:
            phonemes = g2p(typo)
            phoneme_str = ' '.join(phonemes)
        except Exception as e:
            phoneme_str = "ERROR"
        line = f"{word} -> {typo} -> {phoneme_str}"
        print(line)
        f.write(line + "\n")

print(f"\nâœ… Done. Output saved to '{output_path}'")
