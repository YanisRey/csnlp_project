import os
import json
import pandas as pd
import numpy as np
from gensim.models import FastText, Word2Vec, KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import spearmanr
from g2p_en import G2p
import re
import nltk
from nltk.corpus import cmudict

nltk.download('cmudict')
cmu_dict = cmudict.dict()

# === Setup ===
g2p = G2p()
results = []

# === Helper functions ===

def simplify_arpabet(pronunciation: str) -> str:
    return re.sub(r'([A-Z]+)[0-2]', r'\1', pronunciation)

def word_to_phonetic(word, simplify=False):
    word_lower = word.lower()
    if word_lower in cmu_dict:
        # Use the first CMU pronunciation variant
        phones = cmu_dict[word_lower][0]
        phonetic_str = " ".join(phones).strip()
    else:
        phones = g2p(word)
        phonetic_str = " ".join(phones).strip()
    
    if simplify:
        phonetic_str = simplify_arpabet(phonetic_str)
    return phonetic_str


def get_vector(model, token):
    try:
        if hasattr(model, "wv") and hasattr(model.wv, "get_vector"):
            if token in model.wv:
                return model.wv[token]
            else:
                return model.wv.get_vector(token, norm=True)
        else:
            if token in model:
                return model[token]
            else:
                return None
    except KeyError:
        return None

def cosine_vecs(vec1, vec2):
    if vec1 is None or vec2 is None:
        return None
    return cosine_similarity([vec1], [vec2])[0][0]

def adapt_model(model, new_tokens):
    if not hasattr(model, 'build_vocab') or not hasattr(model, 'train'):
        return False
    missing = [t for t in new_tokens if t not in model.wv]
    if not missing:
        return True
    print(f"  Adapting model with {len(missing)} new tokens...")
    model.build_vocab([missing], update=True)
    model.train([missing], total_examples=len(missing), epochs=1)
    return True


def load_embedding_model(path):
    try:
        # First try loading as regular model (Word2Vec/FastText)
        model = Word2Vec.load(path)
        return model
    except (TypeError, AttributeError):
        try:
            # If that fails, try loading as KeyedVectors
            return KeyedVectors.load(path)
        except:
            # Finally try word2vec format
            try:
                return KeyedVectors.load_word2vec_format(
                    path, 
                    binary=path.endswith(".bin")
                )
            except:
                raise ValueError(f"Could not load model from {path} with any supported method")

def normalize(vec, c):
    if vec is None:
        return None
    norm = np.linalg.norm(vec)
    return c*vec / norm if norm > 0 else vec

def combined_vector(w_model, p_model, w1, w2, simplify):
    p1 = word_to_phonetic(w1, simplify)
    p2 = word_to_phonetic(w2, simplify)
    pv1 = normalize(get_vector(p_model, p1),1)
    pv2 = normalize(get_vector(p_model, p2),1)
    wv1 = normalize(get_vector(w_model, w1),4)
    wv2 = normalize(get_vector(w_model, w2),4)
    if any(v is None for v in (pv1, pv2, wv1, wv2)):
        return None, None
    vec1 = np.concatenate([wv1, pv1])
    vec2 = np.concatenate([wv2, pv2])
    return vec1, vec2

# === Load Models ===
print("📦 Loading models...")
model_paths = {
    "fasttext_phonetic": "../results/trained_embeddings/phonetics_models/fasttext_phonetic.model",
    "fasttext_simplified_phonetic": "../results/trained_embeddings/phonetics_models/fasttext_simplified_phonetic.model",
    "fasttext_word": "../results/trained_embeddings/word_models/fasttext_word.model",
    "word2vec_glove": "../results/trained_embeddings/word_models/word2vec_glove.model",
    "spelling_aware_fasttext_word": "../results/trained_embeddings/word_models/fasttext_word_spellingaware.model"
}

loaded_models = {}
for name, path in model_paths.items():
    try:
        loaded_models[name] = load_embedding_model(path)
        print(f"✅ Loaded {name}")
    except Exception as e:
        print(f"❌ Failed to load {name}: {e}")

# === Evaluate on Human-Scored Datasets ===
print("\n🔍 Evaluating on human-scored word similarity datasets...")
human_dir = "../data/human_scored_word_pairs"
human_files = [f for f in os.listdir(human_dir) if f.endswith(".txt")]
print(f"📁 Found {len(human_files)} datasets.")

all_results = []

# First evaluate individual models
individual_models = [
    ("fasttext_phonetic", False),
    ("fasttext_simplified_phonetic", True),
    ("fasttext_word", False),
    ("word2vec_glove", False)
]

for filename in human_files:
    dataset_name = filename.replace(".txt", "")
    file_path = os.path.join(human_dir, filename)
    print(f"\n📄 Processing dataset: {dataset_name}")

    try:
        df = pd.read_csv(file_path, sep=None, engine="python", usecols=[0, 1, 2], names=["word1", "word2", "score"], header=None)
    except Exception as e:
        print(f"❌ Failed to read {filename}: {e}")
        continue

    # Test individual models
    for model_name, simplify in individual_models:
        print(f"   🧠 Evaluating with model: {model_name}")
        model = loaded_models.get(model_name)
        if model is None:
            print(f"      ⚠️ Model {model_name} not loaded; skipping.")
            continue

        sims = []
        gold_scores = []
        skipped = 0

        for _, row in df.iterrows():
            w1, w2, score = row['word1'], row['word2'], row['score']
            
            if "phonetic" in model_name:
                # For phonetic models, we need to convert words to phonetic representations first
                p1 = word_to_phonetic(w1, simplify)
                p2 = word_to_phonetic(w2, simplify)
                v1 = get_vector(model, p1)
                v2 = get_vector(model, p2)
            else:
                # For word models, use the words directly
                v1 = get_vector(model, w1)
                v2 = get_vector(model, w2)
                
            if v1 is None or v2 is None:
                skipped += 1
                continue
                
            sim = cosine_vecs(v1, v2)
            if sim is not None:
                sims.append(sim)
                gold_scores.append(score)

        if not sims:
            print(f"      ⚠️ No valid similarities computed.")
            continue

        corr, _ = spearmanr(sims, gold_scores)
        print(f"      ✅ Spearman correlation: {corr:.4f} (used {len(sims)}/{len(df)} pairs)")

        all_results.append({
            "dataset": dataset_name,
            "model": model_name,
            "spearman_corr": corr,
            "used_pairs": len(sims),
            "total_pairs": len(df),
            "skipped_pairs": skipped
        })

    # === Combined Models ===
    combinations = [
        ("fasttext_word", "fasttext_phonetic", False),
        ("fasttext_word", "fasttext_simplified_phonetic", True),
        ("word2vec_glove", "fasttext_phonetic", False),
        ("word2vec_glove", "fasttext_simplified_phonetic", True)
    ]

    for word_model_name, phon_model_name, simplify in combinations:
        print(f"   🧠 Evaluating combined model: {word_model_name} + {phon_model_name}")
        word_model = loaded_models.get(word_model_name)
        phon_model = loaded_models.get(phon_model_name)
        if word_model is None or phon_model is None:
            print("      ⚠️ One of the models not loaded; skipping.")
            continue

        sims = []
        gold_scores = []
        skipped = 0

        for _, row in df.iterrows():
            w1, w2, score = row['word1'], row['word2'], row['score']
            vec1, vec2 = combined_vector(word_model, phon_model, w1, w2, simplify)
            if vec1 is None or vec2 is None:
                skipped += 1
                continue
            sim = cosine_vecs(vec1, vec2)
            if sim is not None:
                sims.append(sim)
                gold_scores.append(score)

        if not sims:
            print(f"      ⚠️ No valid similarities computed for combined model.")
            continue

        corr, _ = spearmanr(sims, gold_scores)
        combined_name = f"combined/{word_model_name}__{phon_model_name}"
        print(f"      ✅ Spearman correlation: {corr:.4f} (used {len(sims)}/{len(df)} pairs)")

        all_results.append({
            "dataset": dataset_name,
            "model": combined_name,
            "spearman_corr": corr,
            "used_pairs": len(sims),
            "total_pairs": len(df),
            "skipped_pairs": skipped
        })

# === Save Results ===
results_df = pd.DataFrame(all_results)
output_path = "human_eval_results.csv"
results_df.to_csv(output_path, index=False)
print(f"\n📊 All results saved to: {output_path}")
print("✅ Done evaluating all models on all datasets.")
"""import os
import json
import pandas as pd
import numpy as np
from gensim.models import FastText, Word2Vec, KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import spearmanr
from g2p_en import G2p
import re

# === Setup ===
g2p = G2p()
results = []

# === Helper functions ===

def simplify_arpabet(pronunciation: str) -> str:
    return re.sub(r'([A-Z]+)[0-2]', r'\1', pronunciation)

def word_to_phonetic(word, simplify=False):
    phones = g2p(word)
    phonetic_str = " ".join(phones).strip()
    if simplify:
        phonetic_str = simplify_arpabet(phonetic_str)
    return phonetic_str

def get_vector(model, token):
    try:
        if hasattr(model, "wv") and hasattr(model.wv, "get_vector"):
            if token in model.wv:
                return model.wv[token]
            else:
                return model.wv.get_vector(token, norm=True)
        else:
            if token in model:
                return model[token]
            else:
                return None
    except KeyError:
        return None

def cosine_vecs(vec1, vec2):
    if vec1 is None or vec2 is None:
        return None
    return cosine_similarity([vec1], [vec2])[0][0]

def adapt_model(model, new_tokens):
    if not hasattr(model, 'build_vocab') or not hasattr(model, 'train'):
        return False
    missing = [t for t in new_tokens if t not in model.wv]
    if not missing:
        return True
    print(f"  Adapting model with {len(missing)} new tokens...")
    model.build_vocab([missing], update=True)
    model.train([missing], total_examples=len(missing), epochs=1)
    return True

def load_embedding_model(path):
    if path.endswith(".model"):
        return Word2Vec.load(path)
    elif path.endswith(".kv") or path.endswith(".txt") or path.endswith(".vec") or path.endswith(".bin"):
        return KeyedVectors.load_word2vec_format(path, binary=path.endswith(".bin"))
    else:
        raise ValueError(f"Unsupported model format: {path}")

def normalize(vec, c):
    if vec is None:
        return None
    norm = np.linalg.norm(vec)
    return c*vec / norm if norm > 0 else vec

def combined_vector(w_model, p_model, w1, w2, simplify):
    p1 = word_to_phonetic(w1, simplify)
    p2 = word_to_phonetic(w2, simplify)
    pv1 = normalize(get_vector(p_model, p1),1)
    pv2 = normalize(get_vector(p_model, p2),1)
    wv1 = normalize(get_vector(w_model, w1),4)
    wv2 = normalize(get_vector(w_model, w2),4)
    if any(v is None for v in (pv1, pv2, wv1, wv2)):
        return None, None
    vec1 = np.concatenate([wv1, pv1])
    vec2 = np.concatenate([wv2, pv2])
    return vec1, vec2

# === Load Models ===
print("📦 Loading models...")
model_paths = {
    "fasttext_phonetic": "../results/trained_embeddings/phonetics_models/fasttext_phonetic.model",
    "fasttext_simplified_phonetic": "../results/trained_embeddings/phonetics_models/fasttext_simplified_phonetic.model",
    "fasttext_word": "../results/trained_embeddings/words_models/fasttext_word.model",
    "word2vec_glove": "../results/trained_embeddings/words_models/word2vec_glove.model"
}

loaded_models = {}
for name, path in model_paths.items():
    try:
        loaded_models[name] = load_embedding_model(path)
        print(f"✅ Loaded {name}")
    except Exception as e:
        print(f"❌ Failed to load {name}: {e}")

# === Evaluate on Human-Scored Datasets ===
print("\n🔍 Evaluating on human-scored word similarity datasets...")
human_dir = "../data/human_scored_word_pairs"
human_files = [f for f in os.listdir(human_dir) if f.endswith(".txt")]
print(f"📁 Found {len(human_files)} datasets.")

all_results = []

for filename in human_files:
    dataset_name = filename.replace(".txt", "")
    file_path = os.path.join(human_dir, filename)
    print(f"\n📄 Processing dataset: {dataset_name}")

    try:
        df = pd.read_csv(file_path, sep=None, engine="python", usecols=[0, 1, 2], names=["word1", "word2", "score"], header=None)
    except Exception as e:
        print(f"❌ Failed to read {filename}: {e}")
        continue

    for model_name, model in loaded_models.items():
        print(f"   🧠 Evaluating with model: {model_name}")
        sims = []
        gold_scores = []
        skipped = 0

        for _, row in df.iterrows():
            w1, w2, score = row['word1'], row['word2'], row['score']
            v1 = get_vector(model, w1)
            v2 = get_vector(model, w2)
            if v1 is None or v2 is None:
                skipped += 1
                continue
            sim = cosine_vecs(v1, v2)
            if sim is not None:
                sims.append(sim)
                gold_scores.append(score)

        if not sims:
            print(f"      ⚠️ No valid similarities computed.")
            continue

        corr, _ = spearmanr(sims, gold_scores)
        print(f"      ✅ Spearman correlation: {corr:.4f} (used {len(sims)}/{len(df)} pairs)")

        all_results.append({
            "dataset": dataset_name,
            "model": model_name,
            "spearman_corr": corr,
            "used_pairs": len(sims),
            "total_pairs": len(df),
            "skipped_pairs": skipped
        })

    # === Combined Models ===
    combinations = [
        ("fasttext_word", "fasttext_phonetic", False),
        ("fasttext_word", "fasttext_simplified_phonetic", True),
        ("word2vec_glove", "fasttext_phonetic", False),
        ("word2vec_glove", "fasttext_simplified_phonetic", True)
    ]

    for word_model_name, phon_model_name, simplify in combinations:
        print(f"   🧠 Evaluating combined model: {word_model_name} + {phon_model_name}")
        word_model = loaded_models.get(word_model_name)
        phon_model = loaded_models.get(phon_model_name)
        if word_model is None or phon_model is None:
            print("      ⚠️ One of the models not loaded; skipping.")
            continue

        sims = []
        gold_scores = []
        skipped = 0

        for _, row in df.iterrows():
            w1, w2, score = row['word1'], row['word2'], row['score']
            vec1, vec2 = combined_vector(word_model, phon_model, w1, w2, simplify)
            if vec1 is None or vec2 is None:
                skipped += 1
                continue
            sim = cosine_vecs(vec1, vec2)
            if sim is not None:
                sims.append(sim)
                gold_scores.append(score)

        if not sims:
            print(f"      ⚠️ No valid similarities computed for combined model.")
            continue

        corr, _ = spearmanr(sims, gold_scores)
        combined_name = f"combined/{word_model_name}__{phon_model_name}"
        print(f"      ✅ Spearman correlation: {corr:.4f} (used {len(sims)}/{len(df)} pairs)")

        all_results.append({
            "dataset": dataset_name,
            "model": combined_name,
            "spearman_corr": corr,
            "used_pairs": len(sims),
            "total_pairs": len(df),
            "skipped_pairs": skipped
        })

# === Save Results ===
results_df = pd.DataFrame(all_results)
output_path = "human_eval_results.csv"
results_df.to_csv(output_path, index=False)
print(f"\n📊 All results saved to: {output_path}")
print("✅ Done evaluating all models on all datasets.")
"""